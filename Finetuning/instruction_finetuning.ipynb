{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Finetuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](dataset_finetune_prev.jpg) \n",
    "Dataset from: https://huggingface.co/datasets/axiong/pmc_llama_instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.8.2\n",
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from matplotlib import pyplot as plt\n",
    "from litgpt import LLM\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from importlib.metadata import version\n",
    "pkgs = [\"matplotlib\", \"numpy\", \"tiktoken\", \"torch\"]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"instruction_data.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "processed_data = [\n",
    "    {\n",
    "        \"instruction\": item[\"instruction\"],\n",
    "        \"input\": item[\"input\"],\n",
    "        \"output\": item[\"output\"]\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "print(\"Number of entries:\", len(processed_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Test set length: 165\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(processed_data)\n",
    "\n",
    "train_ratio = 0.85\n",
    "train_size = int(len(processed_data) * train_ratio)\n",
    "\n",
    "train_data = processed_data[:train_size]\n",
    "test_data = processed_data[train_size:]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Test set length:\", len(test_data))\n",
    "\n",
    "\n",
    "with open(\"train.json\", \"w\") as json_file:\n",
    "    json.dump(train_data, json_file, indent=4)\n",
    "    \n",
    "with open(\"test.json\", \"w\") as json_file:\n",
    "    json.dump(test_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'checkpoint_dir': PosixPath('checkpoints/microsoft/phi-2'),\n",
      " 'data': JSON(json_path=PosixPath('train.json'),\n",
      "              mask_prompt=False,\n",
      "              val_split_fraction=0.1,\n",
      "              prompt_style=<litgpt.prompts.Alpaca object at 0x7f88cb5a5a20>,\n",
      "              ignore_index=-100,\n",
      "              seed=42,\n",
      "              num_workers=4),\n",
      " 'devices': 1,\n",
      " 'eval': EvalArgs(interval=100,\n",
      "                  max_new_tokens=100,\n",
      "                  max_iters=100,\n",
      "                  initial_validation=False,\n",
      "                  final_validation=True),\n",
      " 'logger_name': 'csv',\n",
      " 'lora_alpha': 16,\n",
      " 'lora_dropout': 0.05,\n",
      " 'lora_head': False,\n",
      " 'lora_key': False,\n",
      " 'lora_mlp': False,\n",
      " 'lora_projection': False,\n",
      " 'lora_query': True,\n",
      " 'lora_r': 8,\n",
      " 'lora_value': True,\n",
      " 'optimizer': 'AdamW',\n",
      " 'out_dir': PosixPath('out/finetune/lora'),\n",
      " 'precision': None,\n",
      " 'quantize': None,\n",
      " 'seed': 1337,\n",
      " 'train': TrainArgs(save_interval=1000,\n",
      "                    log_interval=100,\n",
      "                    global_batch_size=16,\n",
      "                    micro_batch_size=1,\n",
      "                    lr_warmup_steps=100,\n",
      "                    lr_warmup_fraction=None,\n",
      "                    epochs=3,\n",
      "                    max_tokens=None,\n",
      "                    max_steps=None,\n",
      "                    max_seq_length=None,\n",
      "                    tie_embeddings=None,\n",
      "                    max_norm=None,\n",
      "                    min_lr=6e-05)}\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "Seed set to 1337\n",
      "Number of trainable parameters: 2,621,440\n",
      "Number of non-trainable parameters: 2,779,683,840\n",
      "The longest sequence length in the train data is 101, the model's maximum sequence length is 101 and context length is 2048\n",
      "Verifying settings ...\n",
      "Missing logger folder: /teamspace/studios/this_studio/out/finetune/lora/logs/csv\n",
      "Epoch 1 | iter 100 step 6 | loss train: 2.393, val: n/a | iter time: 82.21 ms\n",
      "Epoch 1 | iter 200 step 12 | loss train: 2.138, val: n/a | iter time: 83.16 ms\n",
      "Epoch 1 | iter 300 step 18 | loss train: 1.908, val: n/a | iter time: 80.77 ms\n",
      "Epoch 1 | iter 400 step 25 | loss train: 1.449, val: n/a | iter time: 85.55 ms (step)\n",
      "Epoch 1 | iter 500 step 31 | loss train: 0.931, val: n/a | iter time: 81.96 ms\n",
      "Epoch 1 | iter 600 step 37 | loss train: 0.657, val: n/a | iter time: 81.52 ms\n",
      "Epoch 1 | iter 700 step 43 | loss train: 0.579, val: n/a | iter time: 81.38 ms\n",
      "Epoch 1 | iter 800 step 50 | loss train: 0.695, val: n/a | iter time: 84.98 ms (step)\n",
      "Epoch 2 | iter 900 step 56 | loss train: 0.537, val: n/a | iter time: 82.43 ms\n",
      "Epoch 2 | iter 1000 step 62 | loss train: 0.473, val: n/a | iter time: 80.72 ms\n",
      "Epoch 2 | iter 1100 step 68 | loss train: 0.462, val: n/a | iter time: 80.93 ms\n",
      "Epoch 2 | iter 1200 step 75 | loss train: 0.527, val: n/a | iter time: 81.72 ms (step)\n",
      "Epoch 2 | iter 1300 step 81 | loss train: 0.505, val: n/a | iter time: 79.20 ms\n",
      "Epoch 2 | iter 1400 step 87 | loss train: 0.510, val: n/a | iter time: 80.38 ms\n",
      "Epoch 2 | iter 1500 step 93 | loss train: 0.578, val: n/a | iter time: 82.84 ms\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 | iter 1600 step 100 | loss train: 0.529, val: n/a | iter time: 82.59 ms (step)\n",
      "Validating ...\n",
      "Recommend a movie for me to watch during the weekend and explain the reason.\n",
      "Length of encoded instruction (45) and eval.max_new_tokens (100) exceeds model.max_seq_length (101) used for training. Skipping example generation for efficiency. The model's supported context size (post-training) is 2048.\n",
      "iter 1600: val loss 0.4743, val time: 5395.57 ms\n",
      "Epoch 3 | iter 1700 step 106 | loss train: 0.478, val: 0.474 | iter time: 79.69 ms\n",
      "Epoch 3 | iter 1800 step 112 | loss train: 0.617, val: 0.474 | iter time: 78.80 ms\n",
      "Epoch 3 | iter 1900 step 118 | loss train: 1.122, val: 0.474 | iter time: 79.49 ms\n",
      "Epoch 3 | iter 2000 step 125 | loss train: 0.773, val: 0.474 | iter time: 84.16 ms (step)\n",
      "Epoch 3 | iter 2100 step 131 | loss train: 0.683, val: 0.474 | iter time: 79.49 ms\n",
      "Epoch 3 | iter 2200 step 137 | loss train: 0.746, val: 0.474 | iter time: 82.68 ms\n",
      "Epoch 3 | iter 2300 step 143 | loss train: 0.710, val: 0.474 | iter time: 82.90 ms\n",
      "Epoch 3 | iter 2400 step 150 | loss train: 0.664, val: 0.474 | iter time: 81.16 ms (step)\n",
      "Epoch 3 | iter 2500 step 156 | loss train: 0.697, val: 0.474 | iter time: 86.41 ms\n",
      "Training time: 213.92s\n",
      "Memory used: 16.76 GB\n",
      "Validating ...\n",
      "Final evaluation | val loss: 0.640 | val ppl: 1.896\n",
      "Saving LoRA weights to '/teamspace/studios/this_studio/out/finetune/lora/final/lit_model.pth.lora'\n",
      "{'checkpoint_dir': PosixPath('/teamspace/studios/this_studio/out/finetune/lora/final'),\n",
      " 'precision': None,\n",
      " 'pretrained_checkpoint_dir': None}\n",
      "Saved merged weights to '/teamspace/studios/this_studio/out/finetune/lora/final/lit_model.pth'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!litgpt finetune_lora microsoft/phi-2 \\\n",
    "--data JSON \\\n",
    "--data.val_split_fraction 0.1 \\\n",
    "--data.json_path train.json \\\n",
    "--train.epochs 3 \\\n",
    "--train.log_interval 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:51<00:00,  3.20it/s]\n"
     ]
    }
   ],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    return instruction_text + input_text\n",
    "\n",
    "llm = LLM.load(\"microsoft/phi-2\")\n",
    "\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    response = llm.generate(format_input(test_data[i]))\n",
    "    test_data[i][\"base_model\"] = response\n",
    "\n",
    "with open(\"test_base_model.json\", \"w\") as json_file:\n",
    "    json.dump(test_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 118/165 [00:34<00:12,  3.81it/s]"
     ]
    }
   ],
   "source": [
    "# del llm\n",
    "llm_finetuned = LLM.load(\"/teamspace/studios/this_studio/out/finetune/lora/final/\")\n",
    "\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    response = llm_finetuned.generate(format_input(test_data[i]))\n",
    "    test_data[i][\"finetuned_model\"] = response\n",
    "\n",
    "\n",
    "with open(\"test_base_and_finetuned_model.json\", \"w\") as json_file:\n",
    "    json.dump(test_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Instruction: Arrange the following events in chronological order: Invention of the airplane, Fall of the Berlin Wall, Discovery of America.\n",
      "Base model output:  The correct order of these events is Discovery of America, Invention of the airplane, Fall of the Berlin Wall.\n",
      "\n",
      "Finetuned model output: Italicize the correct order in the following sentence.\n",
      "\n",
      "### Response:\n",
      "Concentrate on providing a good answer that appropriately completes the request.\n",
      "\n",
      "\n",
      "Sample 2:\n",
      "Instruction: Find a synonym for the given verb.\n",
      "Base model output:  Start\n",
      "\n",
      "Finetuned model output: Prologue\n",
      "\n",
      "\n",
      "Sample 3:\n",
      "Instruction: Translate the phrase 'Life is beautiful' into Italian.\n",
      "Base model output:  L'aiei sono belli.\n",
      "\n",
      "Finetuned model output: Lifo semplice\n",
      "\n",
      "\n",
      "Sample 4:\n",
      "Instruction: Convert the following verb to its gerund form: 'eat'\n",
      "Base model output:  Eating\n",
      "\n",
      "Finetuned model output: Eating\n",
      "\n",
      "\n",
      "\n",
      "Sample 5:\n",
      "Instruction: Look up the freezing point of water.\n",
      "Base model output:  The freezing point of water is 0 degrees Celsius or 32 degrees Fahrenheit.\n",
      "\n",
      "Finetuned model output: Here is an input that provides a statement that describes a task.\n",
      "\n",
      "###Irlchemia has written a text that begins with 'What is the freezing point of water?'\n",
      "\n",
      "### Response:\n",
      "The freezing point of water is 0 degrees\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"Instruction:\", test_data[i][\"instruction\"])\n",
    "    print(\"Base model output:\", test_data[i][\"base_model\"])\n",
    "    print(\"Finetuned model output:\", test_data[i][\"finetuned_model\"])\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
